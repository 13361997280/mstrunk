package data.task;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONArray;
import com.alibaba.fastjson.JSONObject;
import com.qbao.search.conf.Config;
import com.qbao.search.conf.LoadValues;
import com.qbao.search.util.HttpClientUtil;
import data.scheduler.DelayQueueScheduler;
import data.service.SpiderService;
import org.apache.commons.httpclient.util.DateUtil;
import org.apache.commons.lang3.StringUtils;
import po.NewsPo;
import us.codecraft.webmagic.Page;
import us.codecraft.webmagic.Site;
import us.codecraft.webmagic.Spider;
import us.codecraft.webmagic.monitor.SpiderMonitor;
import us.codecraft.webmagic.pipeline.ConsolePipeline;
import us.codecraft.webmagic.pipeline.FilePipeline;
import us.codecraft.webmagic.pipeline.Pipeline;
import us.codecraft.webmagic.processor.PageProcessor;
import us.codecraft.webmagic.scheduler.FileCacheQueueScheduler;
import us.codecraft.webmagic.scheduler.Scheduler;

import javax.management.JMException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.TimeUnit;


/**
 * 抓取今日关注热点新闻
 * 
 */
public class NewLinksProcesser implements PageProcessor {

	private static Site site = Site.me().setDomain("toutiao.com");
	public NewLinksProcesser() {
		super();
		site.setCharset("utf-8");
	}

	@Override
	public void process(Page page) {
		long nowTime = System.currentTimeMillis();
		// 得到翻页links
		JSONObject jsonObject = JSON.parseObject(page.getRawText());
		JSONArray jsonArray = jsonObject.getJSONArray("data");
		for(int i=0;i<jsonArray.size();i++){
			JSONObject jsonObject1 = jsonArray.getJSONObject(i);
			String imageUrl = jsonObject1.getString("image_url");
			String newsOrigin = jsonObject1.getString("source");
			String newsUrl = jsonObject1.getString("source_url");
			String news_type = jsonObject1.getString("chinese_tag");
			Long hotTime = jsonObject1.getLong("behot_time")*1000;
			String title = jsonObject1.getString("title");
			String news_tags = jsonObject1.getString("tag");
			if(!newsUrl.contains("group")) continue;
			if(news_type==null||news_type.contains("图片")) continue;
			newsUrl = newsUrl.substring(0,newsUrl.length()-1);
			newsUrl = newsUrl.replaceAll("group/","a");
			String crawl_date_time = DateUtil.formatDate(new Date(),"yyyy-MM-dd HH:mm:ss");
			String newsId =  newsUrl.substring(newsUrl.lastIndexOf("/") + 1,newsUrl.length()-1);
			Map<String,Object> ret = new HashMap<String,Object>();
			ret.put("news_id",newsId);
			ret.put("image_url",imageUrl);
			ret.put("news_url",newsUrl);
			ret.put("news_origin",newsOrigin);
			ret.put("title",title);
			ret.put("news_type",news_type);
			ret.put("news_tags",news_tags);
			ret.put("crawl_date_time",crawl_date_time);
			ret.put("hot_time",new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date(hotTime)));
			spiderDetail(ret);
			if(ret.get("news_time")==null||"".equals(ret.get("news_time"))) continue;
			try {
				SpiderService.getInstance().importEsFromSpider(ret);
			} catch (Exception e) {
				e.printStackTrace();
			}
		}
		System.out.println("++++++++++++++++++Process page time =   " + (System.currentTimeMillis() - nowTime) + "");
	}
	public void spiderDetail(Map<String,Object> ret) {
		String contentUrl = ret.get("news_url").toString();
		String hrmlStr = HttpClientUtil.getPage("http://www.toutiao.com"+contentUrl);
		if (StringUtils.isNotEmpty(hrmlStr)) {
			NewsPo po = new NewsPo();
			po.parser(hrmlStr);
			ret.put("news_website",po.getNews_website());
			ret.put("news_time",po.getNews_time());
			ret.put("content",po.getContent());
			ret.put("content_fenci",po.getContent_fenci());
			//ret.put("share_num",po.getShare_num());
			//ret.put("comment_num",po.getComment_num());
			//ret.put("applause_num",po.getApplause_num());
		}
	}

	@Override
	public Site getSite() {
		return site;
	}
	public static void doProcess(){
		String[] seedLinks = { Config.getBase().get(LoadValues.SPIDER_URL).trim()};
		NewLinksProcesser process = new NewLinksProcesser();
		String projectPath = Config.getBase().get(LoadValues.SPIDER_FOLDRE).trim();
		Pipeline filePipe = new FilePipeline(projectPath);
		Scheduler scheduler = new FileCacheQueueScheduler(projectPath);
		//Scheduler scheduler = new DelayQueueScheduler(10, TimeUnit.SECONDS);
		Spider spider = Spider.create(process).setExitWhenComplete(true)
				.scheduler(scheduler)
				.addUrl(seedLinks)
				.addPipeline(filePipe)
				.addPipeline(new ConsolePipeline())
				.thread(6);

		try {
			SpiderMonitor monitor = SpiderMonitor.instance().register(spider);
			SpiderMonitor.MonitorSpiderListener a = monitor.new MonitorSpiderListener();
			System.out.println("正确数量"+a.getSuccessCount());
			System.out.println("错误数量"+a.getErrorCount());
		} catch (JMException e) {
			e.printStackTrace();
		}
		spider.start();
	}

	public static void main(String[] args) {
		doProcess();

	}
}
